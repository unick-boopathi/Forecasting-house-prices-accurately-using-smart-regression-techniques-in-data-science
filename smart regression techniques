# 1. Import Required Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

import warnings
warnings.filterwarnings('ignore')
# 2. Load Dataset
# Example CSV: replace with your path or use pd.read_csv('train.csv') if using Kaggle data
df = pd.read_csv("house_prices.csv")  # Replace with the correct file
df.head()
# 3. Data Preprocessing
# Drop columns with too many missing values
missing_threshold = 0.4
df = df[df.columns[df.isnull().mean() < missing_threshold]]

# Separate target
y = df['SalePrice']
X = df.drop(['SalePrice', 'Id'], axis=1)

# Separate column types
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

# Preprocessing pipeline
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])
# 4. Split Dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 5. Build and Train Models
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42)
}

results = {}

for name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', model)])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    
    results[name] = {
        "MAE": mean_absolute_error(y_test, y_pred),
        "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
        "R2": r2_score(y_test, y_pred)
    }
# 6. Visualize Results
results_df = pd.DataFrame(results).T
results_df.plot(kind='bar', figsize=(10,6), title="Model Performance Comparison")
plt.ylabel("Error Score")
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

results_df
# 7. Feature Importance from Random Forest
rf_model = Pipeline(steps=[('preprocessor', preprocessor),
                           ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))])
rf_model.fit(X_train, y_train)

# Extract feature names after one-hot encoding
feature_names = rf_model.named_steps['preprocessor'].transformers_[0][2] + \
    rf_model.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features).tolist()

importances = rf_model.named_steps['regressor'].feature_importances_
feat_imp_df = pd.DataFrame({"Feature": feature_names, "Importance": importances})
feat_imp_df = feat_imp_df.sort_values(by="Importance", ascending=False).head(20)

# Plot
plt.figure(figsize=(10,6))
sns.barplot(data=feat_imp_df, x="Importance", y="Feature")
plt.title("Top 20 Important Features")
plt.show()
